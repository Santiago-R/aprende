{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santiago-R/aupa.ai/blob/main/lm-hackers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [A hacker's guide to Language Models](https://colab.research.google.com/github/fastai/lm-hackers/blob/main/lm-hackers.ipynb#scrollTo=0b017bfc-5be0-4e41-9fa1-9f685c3b0de5)\n"
      ],
      "metadata": {
        "id": "8PCrJ_dZIOiQ"
      },
      "id": "8PCrJ_dZIOiQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "pbMRhlx6ffxA"
      },
      "id": "pbMRhlx6ffxA"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # , force_remount=True)"
      ],
      "metadata": {
        "id": "lKUyTtKkduDt",
        "outputId": "7668a931-c91f-4c09-b661-507ab6ffe8e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lKUyTtKkduDt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tokenize\n",
        "# from io import BytesIO"
      ],
      "metadata": {
        "id": "mnQzskuV91G4"
      },
      "id": "mnQzskuV91G4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# path = Path(\"/content/drive/MyDrive/LLM\")"
      ],
      "metadata": {
        "id": "r6yIdsdmfkCS"
      },
      "id": "r6yIdsdmfkCS",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "101f8f63-6a03-49c0-81b8-9173563f7420",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "101f8f63-6a03-49c0-81b8-9173563f7420"
      },
      "source": [
        "## The OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OpenAI api key as environvent variable (from Drive's api_keys.env)\n",
        "!pip install python-dotenv -qq\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path='/content/drive/MyDrive/LLM/api_keys.env')"
      ],
      "metadata": {
        "id": "pHmWz-Hv_bMg",
        "outputId": "bd604de4-3c14-4d1c-df15-df6348fefdd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pHmWz-Hv_bMg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65",
      "metadata": {
        "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qq\n",
        "from openai import ChatCompletion, Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef432d50-54c5-46da-af51-b01488b7983f",
      "metadata": {
        "id": "ef432d50-54c5-46da-af51-b01488b7983f"
      },
      "outputs": [],
      "source": [
        "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
        "question = \"What is money?\"\n",
        "\n",
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": question}])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db35a96f-20a1-4643-a622-91e35a03ab0b",
      "metadata": {
        "id": "db35a96f-20a1-4643-a622-91e35a03ab0b"
      },
      "source": [
        "- [Model options](https://platform.openai.com/docs/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f",
      "metadata": {
        "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f"
      },
      "outputs": [],
      "source": [
        "def response(c):\n",
        "    try:\n",
        "        return c['choices'][0]['message']['content']\n",
        "    except KeyError:\n",
        "        return c['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
      "metadata": {
        "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
        "outputId": "0b71404b-e30b-45eb-c45e-b0b40230898c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, money mate, it's the honey that makes the world go â€˜round! It's the cold hard cash, the moolah, the green stuff we use to buy things we want and need. Think of money as the fuel that keeps the economic engine running. It's how we show our value and exchange goods and services. Without money, well, it'd be like trying to surf without a wave, a real wipeout! So, remember to keep an eye on your dollars and cents, mate!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
      "metadata": {
        "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
        "outputId": "2f9fd7ff-04d6-4b46-da83-39c18e075885",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prompt_tokens\": 31,\n",
            "  \"completion_tokens\": 106,\n",
            "  \"total_tokens\": 137\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(c.usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
      "metadata": {
        "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
        "outputId": "69b69914-7775-479c-8a06-beb118f7f81b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "0.002 / 1000 * 150  # GPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
      "metadata": {
        "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
        "outputId": "307ec7dc-4196-4190-e5dd-b4be10cbbd89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0045"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "0.03 / 1000 * 150  # GPT 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4",
      "metadata": {
        "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
        "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
      "metadata": {
        "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
        "outputId": "fe503388-9988-4629-e0c7-e62524772623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, let me explain, cobber. See, just like kangaroos hop around and help you get from one place to another, money helps you get the things you need and want in life. It\\'s like the fuel that keeps the economic engine running. You work hard, earn those dollars, and then use them to buy stuff, pay your bills, and save up for the future. Money is a way of exchanging value, mate. It\\'s a way of saying, \"Hey, I\\'ve put in the effort, so give me something in return.\" It\\'s a powerful tool, no doubt about it. Just remember, like those kangaroos, money can jump around pretty fast, so it\\'s important to manage it well and make smart decisions along the way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "91029d40-8360-4d0f-9f42-9ac7498cf417",
      "metadata": {
        "id": "91029d40-8360-4d0f-9f42-9ac7498cf417"
      },
      "outputs": [],
      "source": [
        "def askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n",
        "    msgs = []\n",
        "    if system: msgs.append({\"role\": \"system\", \"content\": system})\n",
        "    msgs.append({\"role\": \"user\", \"content\": user})\n",
        "    return ChatCompletion.create(model=model, messages=msgs, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
      "metadata": {
        "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
        "outputId": "96a30c1d-5f8f-4a37-c6fd-5eddbfe7e417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, the meaning of life, mate! It's a perennial question, ain't it? Well, in true Aussie style, I reckon the meaning of life is like a barbie on a sunny arvo - everyone's got their own way of cookin' it up.\\n\\nSome folks reckon the meaning of life is all about findin' happiness and joy. They chase their dreams, soak up life's little pleasures, and make the most of every moment like that first sip of cold beer on a scorchin' hot day.\\n\\nOthers believe the meaning of life is all about makin' a difference in the world, helpin' others, and leavin' a positive mark. It's like bein' a legend who throws a lifesaver to someone strugglin' in life's rough waters.\\n\\nThen there are those who reckon life's all about self-discovery and personal growth. They dive deep into their own psyche, like explorin' the vast outback, searchin' for truth and understanding.\\n\\nBut let me tell ya, mate, at the end of the day, the meaning of life is really what you make of it. It's findin' your own purpose, livin' by your own values, and embracin' the ride, no matter how bumpy it gets. So, grab a cold one, throw some snags on the barbie, and enjoy the journey, my friend!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "response(askgpt('What is the meaning of life?', system=aussie_sys))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99",
      "metadata": {
        "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99"
      },
      "source": [
        "- [Limits](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n",
        "\n",
        "Created by Bing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6",
      "metadata": {
        "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6"
      },
      "outputs": [],
      "source": [
        "def call_api(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    try: return ChatCompletion.create(model=model, messages=msgs)\n",
        "    except openai.error.RateLimitError as e:\n",
        "        retry_after = int(e.headers.get(\"retry-after\", 60))\n",
        "        print(f\"Rate limit exceeded, waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return call_api(params, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
      "metadata": {
        "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
        "outputId": "99872e2d-abd5-4056-e757-9536b6921e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Determining the world\\'s funniest joke is subjective and can vary depending on personal preferences and cultural backgrounds. However, there have been scientific attempts to analyze humor and identify jokes that are widely appreciated. One such study was conducted by Richard Wiseman, a British psychologist, who asked over 40,000 individuals from different countries to rate jokes and identify the funniest one. The study led to the following joke being recognized as the world\\'s funniest:\\n\\n\"Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing, and his eyes are glazed. The other guy whips out his phone and calls emergency services. He gasps, \\'My friend is dead! What can I do?\\' The operator says \\'Calm down, I can help. First, let\\'s make sure he\\'s dead.\\' There\\'s a silence, then a gunshot is heard. Back on the phone, the guy says \\'Okay, now what?\\'\"\\n\\nWhile this joke was identified as the funniest joke in the study, humor is highly subjective, and what one person finds hilarious, another may not.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response(call_api(\"What's the world's funniest joke? Has there ever been any scientific analysis?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "739e1549-687b-4216-bdf0-175e60c57401",
      "metadata": {
        "id": "739e1549-687b-4216-bdf0-175e60c57401"
      },
      "outputs": [],
      "source": [
        "c = Completion.create(prompt=\"Australian Jeremy Howard is \",\n",
        "                      model=\"gpt-3.5-turbo-instruct\", echo=True, logprobs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response(c)"
      ],
      "metadata": {
        "id": "HvAnyswSduzm",
        "outputId": "0b6e15f8-4763-48e1-db85-259ab9ca2b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "id": "HvAnyswSduzm",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Australian Jeremy Howard is warmly received by Kiwis\\n\\nI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e86e7d-64f0-411d-bfce-289b06174dd4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "98e86e7d-64f0-411d-bfce-289b06174dd4"
      },
      "source": [
        "## Create our own code interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785",
      "metadata": {
        "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785"
      },
      "outputs": [],
      "source": [
        "from pydantic import create_model\n",
        "import inspect, json\n",
        "from inspect import Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with sums function"
      ],
      "metadata": {
        "id": "I61saoTTfh35"
      },
      "id": "I61saoTTfh35"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68",
      "metadata": {
        "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68"
      },
      "outputs": [],
      "source": [
        "def sums(a:int, b:int=1):\n",
        "    \"Adds a + b\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5afad9cb-1330-478b-8701-8d275bc1f985",
      "metadata": {
        "id": "5afad9cb-1330-478b-8701-8d275bc1f985"
      },
      "outputs": [],
      "source": [
        "def schema(f):\n",
        "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
        "          for n,o in inspect.signature(f).parameters.items()}\n",
        "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
        "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
      "metadata": {
        "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
        "outputId": "2383902d-f1c3-4669-9518-cff7e2c38041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'sums',\n",
              " 'description': 'Adds a + b',\n",
              " 'parameters': {'title': 'Input for `sums`',\n",
              "  'type': 'object',\n",
              "  'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
              "   'b': {'title': 'B', 'default': 1, 'type': 'integer'}},\n",
              "  'required': ['a']}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "schema(sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453",
      "metadata": {
        "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n",
        "           system = \"You must use the `sum` function instead of adding yourself.\",\n",
        "           functions=[schema(sums)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5mfzxAM5UUb",
        "outputId": "5750ecca-21b3-4c5f-918c-73b0a35d7bc2"
      },
      "id": "Z5mfzxAM5UUb",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-84WKgE3qf5b6QEJjRi9fspd5K4UHt at 0x796e48017330> JSON: {\n",
              "  \"id\": \"chatcmpl-84WKgE3qf5b6QEJjRi9fspd5K4UHt\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1696088290,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": null,\n",
              "        \"function_call\": {\n",
              "          \"name\": \"sums\",\n",
              "          \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "        }\n",
              "      },\n",
              "      \"finish_reason\": \"function_call\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 83,\n",
              "    \"completion_tokens\": 22,\n",
              "    \"total_tokens\": 105\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
      "metadata": {
        "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
        "outputId": "ccfb5e6c-ffa3-4d44-9222-012ee7c3727d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject at 0x796e481d3dd0> JSON: {\n",
              "  \"role\": \"assistant\",\n",
              "  \"content\": null,\n",
              "  \"function_call\": {\n",
              "    \"name\": \"sums\",\n",
              "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "m = c.choices[0].message\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
      "metadata": {
        "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
        "outputId": "46aadb0c-da97-42be-cde6-f669ab6a3a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"a\": 6,\n",
            "  \"b\": 3\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "k = m.function_call.arguments\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4",
      "metadata": {
        "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4"
      },
      "outputs": [],
      "source": [
        "funcs_ok = {'sums', 'python'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9853a78e-8643-4b5b-9b4a-a360c8443344",
      "metadata": {
        "id": "9853a78e-8643-4b5b-9b4a-a360c8443344"
      },
      "outputs": [],
      "source": [
        "def call_func(c):\n",
        "    fc = c.choices[0].message.function_call\n",
        "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
        "    f = globals()[fc.name]\n",
        "    return f(**json.loads(fc.arguments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
      "metadata": {
        "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
        "outputId": "44971309-a158-4cb2-93e9-61dfc819ca60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2",
      "metadata": {
        "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def run(code):\n",
        "    tree = ast.parse(code)\n",
        "    last_node = tree.body[-1] if tree.body else None\n",
        "\n",
        "    # If the last node is an expression, modify the AST to capture the result\n",
        "    if isinstance(last_node, ast.Expr):\n",
        "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
        "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
        "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
        "\n",
        "    ns = {}\n",
        "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
        "    return ns.get('_result', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "37aa6983-d9af-40e8-b438-ed8573399020",
      "metadata": {
        "id": "37aa6983-d9af-40e8-b438-ed8573399020",
        "outputId": "0b7abcab-ee00-482d-a4a7-7a50562507bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "run(\"\"\"\n",
        "a=1\n",
        "b=2\n",
        "a+b\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d",
      "metadata": {
        "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d"
      },
      "outputs": [],
      "source": [
        "def python(code:str):\n",
        "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
        "    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
        "    if go.lower()!='y': return '#FAIL#'\n",
        "    return run(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b",
      "metadata": {
        "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
      "metadata": {
        "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
        "outputId": "50dc6e7a-6d32-40db-8b08-75ac399cb11b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proceed with execution?\n",
            "```\n",
            "import math\n",
            "math.factorial(12)\n",
            "```\n",
            "y\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479001600"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Python's exec & eval"
      ],
      "metadata": {
        "id": "89hOEyvHXIlI"
      },
      "id": "89hOEyvHXIlI"
    },
    {
      "cell_type": "code",
      "source": [
        "exec_schema = {'name': 'exec',\n",
        " 'description': 'Execute the given source Python code',\n",
        " 'parameters': {'title': 'Input for `exec`',\n",
        "  'type': 'object',\n",
        "  'properties': {'source': {'title': 'S', 'type': 'string'}},\n",
        "  'required': ['source']}}"
      ],
      "metadata": {
        "id": "pVDtEgUaDLGy"
      },
      "id": "pVDtEgUaDLGy",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def code_response(c, repl=True):\n",
        "    txt_out = response(c)\n",
        "    if txt_out == None: txt_out = ''\n",
        "    if 'function_call' not in c['choices'][0]['message'].keys():\n",
        "        print(txt_out)\n",
        "        return  # No code output\n",
        "    code = c['choices'][0]['message']['function_call']['arguments']\n",
        "    if code[0] == '{': code = json.loads(code)['source']\n",
        "    txt_out += f'\\n==========\\n{code}\\n==========\\n>>> '\n",
        "    if repl:\n",
        "        code_body = '\\n'.join(code.split('\\n')[:-1])\n",
        "        code_footer = code.split('\\n')[-1]\n",
        "        exec(code_body, locals())\n",
        "        result = eval(code_footer, locals())\n",
        "        txt_out += str(result)\n",
        "    else:\n",
        "        exec(code, locals())\n",
        "        result = None\n",
        "    print(txt_out)\n",
        "    return result"
      ],
      "metadata": {
        "id": "CZIE3OaTEXKP"
      },
      "id": "CZIE3OaTEXKP",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[exec_schema])"
      ],
      "metadata": {
        "id": "wlxgggW8hIO-"
      },
      "id": "wlxgggW8hIO-",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factorial_result = code_response(c, repl=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQUn1nsnbuqn",
        "outputId": "0851769f-8425-40b4-a7d9-3a533108f4de"
      },
      "id": "AQUn1nsnbuqn",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========\n",
            "import math\n",
            "\n",
            "factorial = math.factorial(12)\n",
            "factorial\n",
            "==========\n",
            ">>> 479001600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "factorial_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6CD-vb7qUPV",
        "outputId": "d96c1a7f-5095-431a-83fc-dba96118eff5"
      },
      "id": "I6CD-vb7qUPV",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479001600"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Using result in later calls"
      ],
      "metadata": {
        "id": "EiQWJStKa8L1"
      },
      "id": "EiQWJStKa8L1"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176",
      "metadata": {
        "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    functions=[exec_schema],\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n",
        "              {\"role\": \"function\", \"name\": \"exec\", \"content\": str(factorial_result)}])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response(c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImVNh39guvwd",
        "outputId": "e0f62daa-0049-4b37-e610-0ec4b633b087"
      },
      "id": "ImVNh39guvwd",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The factorial of 12 is 479,001,600.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### And we didn't break its basic use!"
      ],
      "metadata": {
        "id": "3j86PEJHbGLf"
      },
      "id": "3j86PEJHbGLf"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "bef5cc05-0531-422b-9367-e7b90d784a54",
      "metadata": {
        "id": "bef5cc05-0531-422b-9367-e7b90d784a54"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is the capital of France?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[exec_schema])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
      "metadata": {
        "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
        "outputId": "c8861b11-60af-4af9-fdea-fa9fded9c0c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "code_response(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04"
      },
      "source": [
        "## PyTorch and Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f",
      "metadata": {
        "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f"
      },
      "outputs": [],
      "source": [
        "from wikipediaapi import Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad",
      "metadata": {
        "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad"
      },
      "outputs": [],
      "source": [
        "wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
        "jh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\n",
        "jh_page = jh_page.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
      "metadata": {
        "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
        "outputId": "af99d29a-51e5-4e21-b5b7-ac126865c4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
            "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
            "Early in the COVID-19 epidemic he was a leading advocate for masking.\n",
            "\n",
            "Early life\n",
            "Howard was born in London, United Kingdom, and move\n"
          ]
        }
      ],
      "source": [
        "print(jh_page[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
      "metadata": {
        "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
        "outputId": "009fa097-2449-4d64-fe94-fcdc50caceb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "613"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(jh_page.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b",
      "metadata": {
        "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b"
      },
      "outputs": [],
      "source": [
        "ques_ctx = f\"\"\"Answer the question with the help of the provided context.\n",
        "\n",
        "## Context\n",
        "\n",
        "{jh_page}\n",
        "\n",
        "## Question\n",
        "\n",
        "{ques}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a",
      "metadata": {
        "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a"
      },
      "outputs": [],
      "source": [
        "res = gen(mk_prompt(ques_ctx), 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
      "metadata": {
        "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
        "outputId": "0ffda081-7c6c-41b3-a754-47d9bc5a522c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Jeremy Howard is an Australian data scientist, entrepreneur, and educator known for his work in deep learning. He is the co-founder of fast.ai, where he teaches courses, develops software, and conducts research in the field. Before co-founding fast.ai, he was the President and Chief Scientist of Kaggle, the CEO of Fastmail and Optimal Decisions Group, and has a background in management consulting.</s>\n"
          ]
        }
      ],
      "source": [
        "print(res[0].split('### Assistant:\\n')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28",
      "metadata": {
        "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e225ca13-1735-42e9-b162-66268fce5218",
      "metadata": {
        "id": "e225ca13-1735-42e9-b162-66268fce5218"
      },
      "outputs": [],
      "source": [
        "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
      "metadata": {
        "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
        "outputId": "60101e5b-882a-46a7-f6bd-16d9f87bf8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
            "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
            "Early in the COVID-19 epidemic he was a leading advocate for masking.\n"
          ]
        }
      ],
      "source": [
        "jh = jh_page.split('\\n\\n')[0]\n",
        "print(jh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed",
      "metadata": {
        "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed"
      },
      "outputs": [],
      "source": [
        "tb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
      "metadata": {
        "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
        "outputId": "d93a72e7-d0d9-41cd-95f5-54568c3cca70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sir Anthony Charles Lynton Blair  (born 6 May 1953) is a British politician who served as Prime Minister of the United Kingdom from 1997 to 2007 and Leader of the Labour Party from 1994 to 2007. He served as Leader of the Opposition from 1994 to 1997 and had various shadow cabinet posts from 1987 to 1994. Blair was Member of Parliament (MP) for Sedgefield from 1983 to 2007. He \n"
          ]
        }
      ],
      "source": [
        "tb = tb_page.split('\\n\\n')[0]\n",
        "print(tb[:380])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3173701f-1533-4eef-b080-3b771f2ba031",
      "metadata": {
        "id": "3173701f-1533-4eef-b080-3b771f2ba031"
      },
      "outputs": [],
      "source": [
        "q_emb,jh_emb,tb_emb = emb_model.encode([ques,jh,tb], convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
      "metadata": {
        "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
        "outputId": "3c7a1380-dd27-4ead-bcda-ffdee93974f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([384])"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tb_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0",
      "metadata": {
        "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
      "metadata": {
        "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
        "outputId": "4a805aac-a8ca-473b-d3c7-bec7b540ab96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7991, device='cuda:0')"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cosine_similarity(q_emb, jh_emb, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
      "metadata": {
        "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
        "outputId": "55679e83-03fd-4a09-919d-fd8aaf88f1f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5315, device='cuda:0')"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cosine_similarity(q_emb, tb_emb, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b"
      },
      "source": [
        "### Private GPTs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037bc313-052d-46b3-8938-fef988126dcb",
      "metadata": {
        "id": "037bc313-052d-46b3-8938-fef988126dcb"
      },
      "source": [
        "- [Sooo many](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1948c419-4f04-4832-848c-a0c52dcc6a90",
      "metadata": {
        "id": "1948c419-4f04-4832-848c-a0c52dcc6a90"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8",
      "metadata": {
        "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f",
      "metadata": {
        "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f"
      },
      "source": [
        "[knowrohit07/know_sql](https://huggingface.co/datasets/knowrohit07/know_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed118caf-3e94-4773-a182-f2d346ced836",
      "metadata": {
        "id": "ed118caf-3e94-4773-a182-f2d346ced836"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset('knowrohit07/know_sql', revision='f33425d13f9e8aab1b46fa945326e9356d6d5726')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
      "metadata": {
        "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
        "outputId": "8e283317-2ed8-480c-930a-69274f972721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 78562\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
      "metadata": {
        "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
        "outputId": "dd29624f-ea3d-485a-97e3-e22033731d70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'What are the hosts of competitions whose theme is not \"Aliens\"?'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn = ds['train']\n",
        "trn[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e",
      "metadata": {
        "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e"
      },
      "source": [
        "`accelerate launch -m axolotl.cli.train sql.yml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
      "metadata": {
        "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
        "outputId": "df3e65ba-e0b9-442e-b219-d38786e901f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'Get the count of competition hosts by theme.'}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tst = dict(**trn[3])\n",
        "tst['question'] = 'Get the count of competition hosts by theme.'\n",
        "tst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd",
      "metadata": {
        "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd"
      },
      "outputs": [],
      "source": [
        "fmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n",
        "\n",
        "USER: {}\n",
        "===\n",
        "{}\n",
        "ASSISTANT:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2",
      "metadata": {
        "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2"
      },
      "outputs": [],
      "source": [
        "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
      "metadata": {
        "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
        "outputId": "1b575504-6d8d-4f02-fa8d-d0415d8e4ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "List all competition hosts sorted in ascending order.\n",
            "ASSISTANT:\n"
          ]
        }
      ],
      "source": [
        "print(sql_prompt(tst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86827af0-6cf0-43d0-b50a-fe080de83f48",
      "metadata": {
        "id": "86827af0-6cf0-43d0-b50a-fe080de83f48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01",
      "metadata": {
        "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01"
      },
      "outputs": [],
      "source": [
        "ax_model = '/home/jhoward/git/ext/axolotl/qlora-out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7082fe-b668-4f11-abf8-548996ea3004",
      "metadata": {
        "id": "7b7082fe-b668-4f11-abf8-548996ea3004"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54",
      "metadata": {
        "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
        "                                             torch_dtype=torch.bfloat16, device_map=0)\n",
        "model = PeftModel.from_pretrained(model, ax_model)\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('sql-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58bc69-6ea6-416a-baa9-db3338451d60",
      "metadata": {
        "id": "8a58bc69-6ea6-416a-baa9-db3338451d60"
      },
      "outputs": [],
      "source": [
        "toks = tokr(sql_prompt(tst), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a057289b-0645-4142-8db2-cc823cc34898",
      "metadata": {
        "id": "a057289b-0645-4142-8db2-cc823cc34898"
      },
      "outputs": [],
      "source": [
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
      "metadata": {
        "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
        "outputId": "028ae5c4-3e65-4794-e4ca-89de969caafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "Get the count of competition hosts by theme.\n",
            "ASSISTANT: SELECT COUNT(Hosts), Theme FROM farm_competition GROUP BY Theme</s>\n"
          ]
        }
      ],
      "source": [
        "print(tokr.batch_decode(res)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991d4a93-dab8-4777-82d2-301c494deba0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "991d4a93-dab8-4777-82d2-301c494deba0"
      },
      "source": [
        "## [llama.cpp](https://github.com/abetlen/llama-cpp-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069",
      "metadata": {
        "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069"
      },
      "source": [
        "[TheBloke/Llama-2-7b-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f33831f-8c71-47dc-a131-9a6733d68198",
      "metadata": {
        "id": "6f33831f-8c71-47dc-a131-9a6733d68198"
      },
      "outputs": [],
      "source": [
        "!pip install llama_cpp_python -qq\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
        "outputId": "c6a951e8-70f6-4027-f580-0ca71b8f57a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d14fb2855a76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, seed, n_ctx, n_batch, n_gpu_layers, main_gpu, tensor_split, rope_freq_base, rope_freq_scale, low_vram, mul_mat_q, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, last_n_tokens_size, lora_base, lora_path, numa, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model path does not exist: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model path does not exist: /home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=\"content/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
      "metadata": {
        "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
        "outputId": "3fe3618d-1edd-4ae1-918d-c0842542addc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   192.25 ms\n",
            "llama_print_timings:      sample time =    14.98 ms /    32 runs   (    0.47 ms per token,  2135.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   192.16 ms /    15 tokens (   12.81 ms per token,    78.06 tokens per second)\n",
            "llama_print_timings:        eval time =   767.74 ms /    31 runs   (   24.77 ms per token,    40.38 tokens per second)\n",
            "llama_print_timings:       total time =  1032.79 ms\n"
          ]
        }
      ],
      "source": [
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
      "metadata": {
        "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
        "outputId": "a31baa72-3c5f-41e7-b2f5-541505766ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'text': 'Q: Name the planets in the solar system? A: 1. Pluto (no longer considered a planet) 2. Mercury 3. Venus 4. Earth 5. Mars 6.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
          ]
        }
      ],
      "source": [
        "print(output['choices'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}