{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santiago-R/aupa.ai/blob/main/lm-hackers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [A hacker's guide to Language Models](https://colab.research.google.com/github/fastai/lm-hackers/blob/main/lm-hackers.ipynb#scrollTo=0b017bfc-5be0-4e41-9fa1-9f685c3b0de5)\n"
      ],
      "metadata": {
        "id": "8PCrJ_dZIOiQ"
      },
      "id": "8PCrJ_dZIOiQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "pbMRhlx6ffxA"
      },
      "id": "pbMRhlx6ffxA"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # , force_remount=True)"
      ],
      "metadata": {
        "id": "lKUyTtKkduDt",
        "outputId": "76721762-dabc-455d-805e-1f40c133be54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lKUyTtKkduDt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tokenize\n",
        "# from io import BytesIO"
      ],
      "metadata": {
        "id": "mnQzskuV91G4"
      },
      "id": "mnQzskuV91G4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# path = Path(\"/content/drive/MyDrive/LLM\")"
      ],
      "metadata": {
        "id": "r6yIdsdmfkCS"
      },
      "id": "r6yIdsdmfkCS",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "101f8f63-6a03-49c0-81b8-9173563f7420",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "101f8f63-6a03-49c0-81b8-9173563f7420"
      },
      "source": [
        "## The OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OpenAI api key as environvent variable (from Drive's api_keys.env)\n",
        "!pip install python-dotenv -qq\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path='/content/drive/MyDrive/LLM/api_keys.env')"
      ],
      "metadata": {
        "id": "pHmWz-Hv_bMg",
        "outputId": "00c2600c-74db-4074-aca3-d69fbaddf56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pHmWz-Hv_bMg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65",
      "metadata": {
        "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adbb97d-06a9-4fa9-b5a4-41d63400f540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai -qq\n",
        "from openai import ChatCompletion, Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef432d50-54c5-46da-af51-b01488b7983f",
      "metadata": {
        "id": "ef432d50-54c5-46da-af51-b01488b7983f"
      },
      "outputs": [],
      "source": [
        "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
        "question = \"What is money?\"\n",
        "\n",
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": question}])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db35a96f-20a1-4643-a622-91e35a03ab0b",
      "metadata": {
        "id": "db35a96f-20a1-4643-a622-91e35a03ab0b"
      },
      "source": [
        "- [Model options](https://platform.openai.com/docs/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f",
      "metadata": {
        "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f"
      },
      "outputs": [],
      "source": [
        "def response(c):\n",
        "    try:\n",
        "        return c['choices'][0]['message']['content']\n",
        "    except KeyError:\n",
        "        return c['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
      "metadata": {
        "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
        "outputId": "57946853-584c-495f-907d-e7b0a174f317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, mate, money is like the lamingtons at a tuck shop, it's the currency we use to sweeten the deal in the big playground of life. It's those crisp notes and shiny coins that make the world go 'round. Money is basically a medium of exchange for goods and services, allowing us to buy stuff we need or want. Think of it as the fuel that keeps the economic engine running, allowing us to grab a snag at the sausage sizzle or snag that awesome new surfboard.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
      "metadata": {
        "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
        "outputId": "038c8098-6643-4640-96a9-434dc33503df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prompt_tokens\": 31,\n",
            "  \"completion_tokens\": 105,\n",
            "  \"total_tokens\": 136\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(c.usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
      "metadata": {
        "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
        "outputId": "21d22313-e685-43e9-a0c4-3fed58af6380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "0.002 / 1000 * 150  # GPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
      "metadata": {
        "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
        "outputId": "d92632e0-7b95-40ac-8dce-20860d336461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0045"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "0.03 / 1000 * 150  # GPT 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4",
      "metadata": {
        "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
        "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
      "metadata": {
        "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
        "outputId": "deedfa1b-2de4-49a4-e35e-6eb225b00641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh, let me break it down for ya, cobber. Just like kangaroos hop and jump around, money helps us navigate through the financial landscape. It's what we use to trade goods and services, mate. Just like kangaroos are the currency of the outback, money is the currency of the economy. Without it, things can get pretty rough, just like a kangaroo caught without its pouch! So, ya see, money is mighty important, just like kangaroos are to our wildlife.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "91029d40-8360-4d0f-9f42-9ac7498cf417",
      "metadata": {
        "id": "91029d40-8360-4d0f-9f42-9ac7498cf417"
      },
      "outputs": [],
      "source": [
        "def askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n",
        "    msgs = []\n",
        "    if system: msgs.append({\"role\": \"system\", \"content\": system})\n",
        "    msgs.append({\"role\": \"user\", \"content\": user})\n",
        "    return ChatCompletion.create(model=model, messages=msgs, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
      "metadata": {
        "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
        "outputId": "becf37aa-c782-4056-8fda-d4f3bdfc90a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, the meaning of life, mate! It's the age-old question that's got philosophers scratching their heads for centuries. Now, I'm no philosopher, but I reckon the meaning of life is about finding your true purpose, your passion, and living a bloody good life while you're at it!\\n\\nJust like a game of footy, life's about kicking goals and enjoying the journey, not just the scoreboard. It's about forming connections, chasing dreams, and making a positive impact. So, find what lights a fire in your belly, pursue it with all you've got, and remember to have a good laugh along the way, because life's too short to be as serious as a mob of kangaroos in a drought!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "response(askgpt('What is the meaning of life?', system=aussie_sys))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99",
      "metadata": {
        "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99"
      },
      "source": [
        "- [Limits](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n",
        "\n",
        "Created by Bing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6",
      "metadata": {
        "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6"
      },
      "outputs": [],
      "source": [
        "def call_api(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    try: return ChatCompletion.create(model=model, messages=msgs)\n",
        "    except openai.error.RateLimitError as e:\n",
        "        retry_after = int(e.headers.get(\"retry-after\", 60))\n",
        "        print(f\"Rate limit exceeded, waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return call_api(params, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
      "metadata": {
        "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
        "outputId": "e498f77f-471f-4673-b257-cdb76c293251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Humor is subjective, and what one person finds funny, another may not. Therefore, it is challenging to determine the world\\'s funniest joke as it varies from person to person and culture to culture.\\n\\nThat being said, there have been attempts to analyze jokes scientifically and determine what elements make them funnier. In 2002, psychologist Richard Wiseman conducted a study called the \"LaughLab project\" in which people submitted jokes and rated them based on humor. The aim was to find the world\\'s funniest joke. The winning joke from this project was as follows:\\n\\n\"Two hunters are out in the woods when one of them collapses. He doesn\\'t seem to be breathing, and his eyes are glazed. The other guy whips out his phone and calls emergency services. He gasps, \\'My friend is dead! What can I do?\\' The operator says \\'Calm down. I can help. First, let\\'s make sure he\\'s dead.\\' There is a silence; then, a gunshot is heard. Back on the phone, the guy says, \\'OK, now what?\\'\"\\n\\nIt is important to note that while this joke was the most popular one in the study, it may not necessarily be the funniest for everyone. Humor is highly subjective and can vary greatly between individuals and cultures.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response(call_api(\"What's the world's funniest joke? Has there ever been any scientific analysis?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "739e1549-687b-4216-bdf0-175e60c57401",
      "metadata": {
        "id": "739e1549-687b-4216-bdf0-175e60c57401"
      },
      "outputs": [],
      "source": [
        "c = Completion.create(prompt=\"Australian Jeremy Howard is \",\n",
        "                      model=\"gpt-3.5-turbo-instruct\", echo=True, logprobs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response(c)"
      ],
      "metadata": {
        "id": "HvAnyswSduzm",
        "outputId": "ec458bac-dd77-4963-b62a-c879313eed95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "id": "HvAnyswSduzm",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Australian Jeremy Howard is one of the leading figures in artificial intelligence. Here, he shares his thoughts about'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e86e7d-64f0-411d-bfce-289b06174dd4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "98e86e7d-64f0-411d-bfce-289b06174dd4"
      },
      "source": [
        "## Create our own code interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785",
      "metadata": {
        "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785"
      },
      "outputs": [],
      "source": [
        "from pydantic import create_model\n",
        "import inspect, json\n",
        "from inspect import Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with sums function"
      ],
      "metadata": {
        "id": "I61saoTTfh35"
      },
      "id": "I61saoTTfh35"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68",
      "metadata": {
        "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68"
      },
      "outputs": [],
      "source": [
        "def sums(a:int, b:int=1):\n",
        "    \"Adds a + b\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "5afad9cb-1330-478b-8701-8d275bc1f985",
      "metadata": {
        "id": "5afad9cb-1330-478b-8701-8d275bc1f985"
      },
      "outputs": [],
      "source": [
        "def schema(f):\n",
        "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
        "          for n,o in inspect.signature(f).parameters.items()}\n",
        "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
        "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
      "metadata": {
        "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
        "outputId": "484df075-5a70-4feb-8735-c068651cc3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'sums',\n",
              " 'description': 'Adds a + b',\n",
              " 'parameters': {'title': 'Input for `sums`',\n",
              "  'type': 'object',\n",
              "  'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
              "   'b': {'title': 'B', 'default': 1, 'type': 'integer'}},\n",
              "  'required': ['a']}}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "schema(sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453",
      "metadata": {
        "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n",
        "           system = \"You must use the `sum` function instead of adding yourself.\",\n",
        "           functions=[schema(sums)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5mfzxAM5UUb",
        "outputId": "f94097fe-7de1-4602-c322-64c98aeb7680"
      },
      "id": "Z5mfzxAM5UUb",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-85cEL3Qec7DCZSuILSJjMAgpqitWy at 0x797f749b5490> JSON: {\n",
              "  \"id\": \"chatcmpl-85cEL3Qec7DCZSuILSJjMAgpqitWy\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1696349289,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": null,\n",
              "        \"function_call\": {\n",
              "          \"name\": \"sums\",\n",
              "          \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "        }\n",
              "      },\n",
              "      \"finish_reason\": \"function_call\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 83,\n",
              "    \"completion_tokens\": 22,\n",
              "    \"total_tokens\": 105\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
      "metadata": {
        "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
        "outputId": "a5d0f13f-0afd-4218-dacb-a90153ed6b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject at 0x797f736b24d0> JSON: {\n",
              "  \"role\": \"assistant\",\n",
              "  \"content\": null,\n",
              "  \"function_call\": {\n",
              "    \"name\": \"sums\",\n",
              "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "m = c.choices[0].message\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
      "metadata": {
        "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
        "outputId": "1c1517f7-f21e-45a9-fc76-e6c51e8ecdb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"a\": 6,\n",
            "  \"b\": 3\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "k = m.function_call.arguments\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4",
      "metadata": {
        "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4"
      },
      "outputs": [],
      "source": [
        "funcs_ok = {'sums', 'python'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "9853a78e-8643-4b5b-9b4a-a360c8443344",
      "metadata": {
        "id": "9853a78e-8643-4b5b-9b4a-a360c8443344"
      },
      "outputs": [],
      "source": [
        "def call_func(c):\n",
        "    fc = c.choices[0].message.function_call\n",
        "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
        "    f = globals()[fc.name]\n",
        "    return f(**json.loads(fc.arguments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
      "metadata": {
        "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
        "outputId": "f07ad588-7881-4927-de43-bd00c189d8c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2",
      "metadata": {
        "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def run(code):\n",
        "    tree = ast.parse(code)\n",
        "    last_node = tree.body[-1] if tree.body else None\n",
        "\n",
        "    # If the last node is an expression, modify the AST to capture the result\n",
        "    if isinstance(last_node, ast.Expr):\n",
        "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
        "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
        "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
        "\n",
        "    ns = {}\n",
        "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
        "    return ns.get('_result', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "37aa6983-d9af-40e8-b438-ed8573399020",
      "metadata": {
        "id": "37aa6983-d9af-40e8-b438-ed8573399020",
        "outputId": "94716b25-e272-447b-f816-96b5be302678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "run(\"\"\"\n",
        "a=1\n",
        "b=2\n",
        "a+b\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d",
      "metadata": {
        "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d"
      },
      "outputs": [],
      "source": [
        "def python(code:str, safe_mode:bool=False):\n",
        "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
        "    if safe_mode:\n",
        "        go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
        "        if go.lower()!='y': return '#FAIL#'\n",
        "    return run(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b",
      "metadata": {
        "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
      "metadata": {
        "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
        "outputId": "467b5c24-0a59-43a2-c000-458b216c7e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479001600"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Python's exec & eval"
      ],
      "metadata": {
        "id": "89hOEyvHXIlI"
      },
      "id": "89hOEyvHXIlI"
    },
    {
      "cell_type": "code",
      "source": [
        "exec_schema = {'name': 'exec',\n",
        " 'description': 'Execute the given source Python code',\n",
        " 'parameters': {'title': 'Input for `exec`',\n",
        "  'type': 'object',\n",
        "  'properties': {'source': {'title': 'S', 'type': 'string'}},\n",
        "  'required': ['source']}}"
      ],
      "metadata": {
        "id": "pVDtEgUaDLGy"
      },
      "id": "pVDtEgUaDLGy",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def code_response(c, repl=True):\n",
        "    txt_out = response(c)\n",
        "    if txt_out == None: txt_out = ''\n",
        "    if 'function_call' not in c['choices'][0]['message'].keys():\n",
        "        print(txt_out)\n",
        "        return  # No code output\n",
        "    code = c['choices'][0]['message']['function_call']['arguments']\n",
        "    if code[0] == '{': code = json.loads(code)['source']\n",
        "    txt_out += f'\\n==========\\n{code}\\n==========\\n>>> '\n",
        "    if repl:\n",
        "        code_body = '\\n'.join(code.split('\\n')[:-1])\n",
        "        code_footer = code.split('\\n')[-1]\n",
        "        exec(code_body, locals())\n",
        "        result = eval(code_footer, locals())\n",
        "        txt_out += str(result)\n",
        "    else:\n",
        "        exec(code, locals())\n",
        "        result = None\n",
        "    print(txt_out)\n",
        "    return result"
      ],
      "metadata": {
        "id": "CZIE3OaTEXKP"
      },
      "id": "CZIE3OaTEXKP",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[exec_schema])"
      ],
      "metadata": {
        "id": "wlxgggW8hIO-"
      },
      "id": "wlxgggW8hIO-",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factorial_result = code_response(c, repl=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQUn1nsnbuqn",
        "outputId": "6d070596-fe91-4b01-bf3c-e069dae6684a"
      },
      "id": "AQUn1nsnbuqn",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========\n",
            "import math\n",
            "\n",
            "factorial = math.factorial(12)\n",
            "factorial\n",
            "==========\n",
            ">>> 479001600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "factorial_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6CD-vb7qUPV",
        "outputId": "b7688d1a-2cf7-488c-fe6e-5d4d016bd100"
      },
      "id": "I6CD-vb7qUPV",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479001600"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Using result in later calls"
      ],
      "metadata": {
        "id": "EiQWJStKa8L1"
      },
      "id": "EiQWJStKa8L1"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176",
      "metadata": {
        "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    functions=[exec_schema],\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n",
        "              {\"role\": \"function\", \"name\": \"exec\", \"content\": str(factorial_result)}])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response(c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImVNh39guvwd",
        "outputId": "da2f32e6-2028-4cdb-9e01-892ddb6c1e99"
      },
      "id": "ImVNh39guvwd",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 factorial, denoted as 12!, is equal to 479,001,600.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### And we didn't break its basic use!"
      ],
      "metadata": {
        "id": "3j86PEJHbGLf"
      },
      "id": "3j86PEJHbGLf"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "bef5cc05-0531-422b-9367-e7b90d784a54",
      "metadata": {
        "id": "bef5cc05-0531-422b-9367-e7b90d784a54"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is the capital of France?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[exec_schema])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
      "metadata": {
        "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
        "outputId": "ef051dfd-7f30-4510-c761-938a54c5f2b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "code_response(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04"
      },
      "source": [
        "## PyTorch and Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "009f4187-ffd6-43ba-b5f1-709b5a3dcf1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b739422-59bf-4147-b562-600f920ee81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers -qq\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "id": "009f4187-ffd6-43ba-b5f1-709b5a3dcf1b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0fa7911-c191-4945-97aa-6daff95970d7"
      },
      "source": [
        "- [HF leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n",
        "- [fasteval](https://fasteval.github.io/FastEval/)"
      ],
      "id": "c0fa7911-c191-4945-97aa-6daff95970d7"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "3e1ed7e9-fbf5-463c-9ff8-2f22c75088bf"
      },
      "outputs": [],
      "source": [
        "mn = \"meta-llama/Llama-2-7b-hf\""
      ],
      "id": "3e1ed7e9-fbf5-463c-9ff8-2f22c75088bf"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "bb16cb3e-0634-4eed-b2b4-1422ecce9cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "c585470a-bd4a-47d8-fcc9-f5f5361092a8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1196\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    276\u001b[0m             )\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-651c3c99-64afc60b56eae9f321ce2a01;92a6f9b8-eeb8-46c9-af2b-3975293035af)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nRepo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-f0323055a0dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         )\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mGatedRepoError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to request access at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id} and pass a token having permission to this repo either \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-hf and pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`."
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, load_in_8bit=True)"
      ],
      "id": "bb16cb3e-0634-4eed-b2b4-1422ecce9cc3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94600cb9-2a64-47e2-aca3-99a72985157f"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained(mn)\n",
        "prompt = \"Jeremy Howard is a \"\n",
        "toks = tokr(prompt, return_tensors=\"pt\")"
      ],
      "id": "94600cb9-2a64-47e2-aca3-99a72985157f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20605293-b31f-4db7-b6a5-0ef662d89441"
      },
      "outputs": [],
      "source": [
        "toks"
      ],
      "id": "20605293-b31f-4db7-b6a5-0ef662d89441"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c20ab287-7d80-47c2-870f-e2ee6bfc4492"
      },
      "outputs": [],
      "source": [
        "tokr.batch_decode(toks['input_ids'])"
      ],
      "id": "c20ab287-7d80-47c2-870f-e2ee6bfc4492"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ab3ad5dc-51b4-48ca-92ae-5027069771c1",
        "outputId": "a5dc8fa3-3cd3-43a1-a467-7f0f741ed192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ],
      "id": "ab3ad5dc-51b4-48ca-92ae-5027069771c1"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "af5a23fc-aa2a-4ce2-92c7-75364130e4e6",
        "outputId": "1b6ed726-4187-4211-e7d3-7ce81aaa5bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-364a2fff925a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokr' is not defined"
          ]
        }
      ],
      "source": [
        "tokr.batch_decode(res)"
      ],
      "id": "af5a23fc-aa2a-4ce2-92c7-75364130e4e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15311da3-bfc0-4453-b4a8-6a9773db626b"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)"
      ],
      "id": "15311da3-bfc0-4453-b4a8-6a9773db626b"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "c15c511b-f5f7-46e2-8fbc-514262117e15",
        "outputId": "b3888d88-0799-41e8-8f3b-13a4b73319c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ],
      "id": "c15c511b-f5f7-46e2-8fbc-514262117e15"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "c581df92-b1cd-4c02-8cde-b2de96d302dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "136e0013-7397-4164-acbd-3dca6f0ef74c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-132a6c9aa8b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TheBloke/Llama-2-7b-Chat-GPTQ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('TheBloke/Llama-2-7b-Chat-GPTQ', device_map=0, torch_dtype=torch.float16)"
      ],
      "id": "c581df92-b1cd-4c02-8cde-b2de96d302dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f47b5b7-ea46-41f0-8338-027b13e0586c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ],
      "id": "8f47b5b7-ea46-41f0-8338-027b13e0586c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb4b60cd-859b-4b3a-b76d-e9eddcdf7cf0"
      },
      "outputs": [],
      "source": [
        "mn = 'TheBloke/Llama-2-13B-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.float16)"
      ],
      "id": "bb4b60cd-859b-4b3a-b76d-e9eddcdf7cf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da90406b-b8a8-4939-a0e9-18c933b62a7c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=15).to('cpu')\n",
        "res"
      ],
      "id": "da90406b-b8a8-4939-a0e9-18c933b62a7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c88d11f8-e41d-4db8-91f2-3e2f695a4d00"
      },
      "outputs": [],
      "source": [
        "def gen(p, maxlen=15, sample=True):\n",
        "    toks = tokr(p, return_tensors=\"pt\")\n",
        "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample).to('cpu')\n",
        "    return tokr.batch_decode(res)"
      ],
      "id": "c88d11f8-e41d-4db8-91f2-3e2f695a4d00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc143703-4fc7-46ce-8c59-cdeec52e7830"
      },
      "outputs": [],
      "source": [
        "gen(prompt, 50)"
      ],
      "id": "dc143703-4fc7-46ce-8c59-cdeec52e7830"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18fb80c0-8664-498c-bb7d-370de8bf6ca7"
      },
      "source": [
        "[StableBeluga-7B](https://huggingface.co/stabilityai/StableBeluga-7B)"
      ],
      "id": "18fb80c0-8664-498c-bb7d-370de8bf6ca7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd343d7b-ef48-4077-bc97-deeec24e324b"
      },
      "outputs": [],
      "source": [
        "mn = \"stabilityai/StableBeluga-7B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.bfloat16)"
      ],
      "id": "dd343d7b-ef48-4077-bc97-deeec24e324b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "432074d6-5d04-4afe-b6cb-06d4412023d6"
      },
      "outputs": [],
      "source": [
        "sb_sys = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can.\\n\\n\""
      ],
      "id": "432074d6-5d04-4afe-b6cb-06d4412023d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1392331-731e-416d-be2d-bc172167e25e"
      },
      "outputs": [],
      "source": [
        "def mk_prompt(user, syst=sb_sys): return f\"{syst}### User: {user}\\n\\n### Assistant:\\n\""
      ],
      "id": "e1392331-731e-416d-be2d-bc172167e25e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a68af5c-5714-4736-9569-5619c54b2bdb"
      },
      "outputs": [],
      "source": [
        "ques = \"Who is Jeremy Howard?\""
      ],
      "id": "5a68af5c-5714-4736-9569-5619c54b2bdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90b5f8e8-a16c-40e6-8dc6-c467ffaa6268"
      },
      "outputs": [],
      "source": [
        "gen(mk_prompt(ques), 150)"
      ],
      "id": "90b5f8e8-a16c-40e6-8dc6-c467ffaa6268"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1a0cfd5-4c64-4591-be83-77b846c41e57"
      },
      "source": [
        "[OpenOrca/Platypus 2](https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B)"
      ],
      "id": "b1a0cfd5-4c64-4591-be83-77b846c41e57"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c7c670a-17aa-41da-8b61-19db6a5dc019"
      },
      "outputs": [],
      "source": [
        "mn = 'TheBloke/OpenOrca-Platypus2-13B-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, torch_dtype=torch.float16)"
      ],
      "id": "6c7c670a-17aa-41da-8b61-19db6a5dc019"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2a267f4-396d-4852-9352-0ca90c0c8441"
      },
      "outputs": [],
      "source": [
        "def mk_oo_prompt(user): return f\"### Instruction: {user}\\n\\n### Response:\\n\""
      ],
      "id": "d2a267f4-396d-4852-9352-0ca90c0c8441"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b931d9c6-69c6-4385-b118-e5ad56781ba6"
      },
      "outputs": [],
      "source": [
        "gen(mk_oo_prompt(ques), 150)"
      ],
      "id": "b931d9c6-69c6-4385-b118-e5ad56781ba6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f",
      "metadata": {
        "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api\n",
        "from wikipediaapi import Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad",
      "metadata": {
        "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad"
      },
      "outputs": [],
      "source": [
        "wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
        "jh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\n",
        "jh_page = jh_page.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
      "metadata": {
        "id": "53a0689a-ab59-431b-838d-33cae6f6500a"
      },
      "outputs": [],
      "source": [
        "print(jh_page[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
      "metadata": {
        "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0"
      },
      "outputs": [],
      "source": [
        "len(jh_page.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b",
      "metadata": {
        "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b"
      },
      "outputs": [],
      "source": [
        "# def mk_prompt_context(question, context):\n",
        "#     return f\"\"\"Answer the question with the help of the provided context.\\n\\n## Context\\n\\n{context}\\n\\n## Question\\n\\n{question}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a",
      "metadata": {
        "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "587a0849-f45e-4550-8655-2b47d69fabca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-a6de63d003b0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmk_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mques_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gen' is not defined"
          ]
        }
      ],
      "source": [
        "res = gen(mk_prompt(ques_ctx), 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
      "metadata": {
        "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2"
      },
      "outputs": [],
      "source": [
        "print(res[0].split('### Assistant:\\n')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28",
      "metadata": {
        "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e225ca13-1735-42e9-b162-66268fce5218",
      "metadata": {
        "id": "e225ca13-1735-42e9-b162-66268fce5218"
      },
      "outputs": [],
      "source": [
        "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
      "metadata": {
        "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4"
      },
      "outputs": [],
      "source": [
        "jh = jh_page.split('\\n\\n')[0]\n",
        "print(jh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed",
      "metadata": {
        "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed"
      },
      "outputs": [],
      "source": [
        "tb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
      "metadata": {
        "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae"
      },
      "outputs": [],
      "source": [
        "tb = tb_page.split('\\n\\n')[0]\n",
        "print(tb[:380])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3173701f-1533-4eef-b080-3b771f2ba031",
      "metadata": {
        "id": "3173701f-1533-4eef-b080-3b771f2ba031"
      },
      "outputs": [],
      "source": [
        "q_emb,jh_emb,tb_emb = emb_model.encode([ques,jh,tb], convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
      "metadata": {
        "id": "6237634d-3708-477d-97e3-34dd7900f4eb"
      },
      "outputs": [],
      "source": [
        "tb_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0",
      "metadata": {
        "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
      "metadata": {
        "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016"
      },
      "outputs": [],
      "source": [
        "F.cosine_similarity(q_emb, jh_emb, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
      "metadata": {
        "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59"
      },
      "outputs": [],
      "source": [
        "F.cosine_similarity(q_emb, tb_emb, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b"
      },
      "source": [
        "### Private GPTs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037bc313-052d-46b3-8938-fef988126dcb",
      "metadata": {
        "id": "037bc313-052d-46b3-8938-fef988126dcb"
      },
      "source": [
        "- [Sooo many](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1948c419-4f04-4832-848c-a0c52dcc6a90",
      "metadata": {
        "id": "1948c419-4f04-4832-848c-a0c52dcc6a90"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8",
      "metadata": {
        "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f",
      "metadata": {
        "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f"
      },
      "source": [
        "[knowrohit07/know_sql](https://huggingface.co/datasets/knowrohit07/know_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed118caf-3e94-4773-a182-f2d346ced836",
      "metadata": {
        "id": "ed118caf-3e94-4773-a182-f2d346ced836"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset('knowrohit07/know_sql', revision='f33425d13f9e8aab1b46fa945326e9356d6d5726')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
      "metadata": {
        "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
        "outputId": "8e283317-2ed8-480c-930a-69274f972721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 78562\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
      "metadata": {
        "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
        "outputId": "dd29624f-ea3d-485a-97e3-e22033731d70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'What are the hosts of competitions whose theme is not \"Aliens\"?'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn = ds['train']\n",
        "trn[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e",
      "metadata": {
        "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e"
      },
      "source": [
        "`accelerate launch -m axolotl.cli.train sql.yml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
      "metadata": {
        "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
        "outputId": "df3e65ba-e0b9-442e-b219-d38786e901f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'Get the count of competition hosts by theme.'}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tst = dict(**trn[3])\n",
        "tst['question'] = 'Get the count of competition hosts by theme.'\n",
        "tst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd",
      "metadata": {
        "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd"
      },
      "outputs": [],
      "source": [
        "fmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n",
        "\n",
        "USER: {}\n",
        "===\n",
        "{}\n",
        "ASSISTANT:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2",
      "metadata": {
        "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2"
      },
      "outputs": [],
      "source": [
        "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
      "metadata": {
        "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
        "outputId": "1b575504-6d8d-4f02-fa8d-d0415d8e4ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "List all competition hosts sorted in ascending order.\n",
            "ASSISTANT:\n"
          ]
        }
      ],
      "source": [
        "print(sql_prompt(tst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86827af0-6cf0-43d0-b50a-fe080de83f48",
      "metadata": {
        "id": "86827af0-6cf0-43d0-b50a-fe080de83f48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01",
      "metadata": {
        "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01"
      },
      "outputs": [],
      "source": [
        "ax_model = '/home/jhoward/git/ext/axolotl/qlora-out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7082fe-b668-4f11-abf8-548996ea3004",
      "metadata": {
        "id": "7b7082fe-b668-4f11-abf8-548996ea3004"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54",
      "metadata": {
        "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
        "                                             torch_dtype=torch.bfloat16, device_map=0)\n",
        "model = PeftModel.from_pretrained(model, ax_model)\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('sql-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58bc69-6ea6-416a-baa9-db3338451d60",
      "metadata": {
        "id": "8a58bc69-6ea6-416a-baa9-db3338451d60"
      },
      "outputs": [],
      "source": [
        "toks = tokr(sql_prompt(tst), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a057289b-0645-4142-8db2-cc823cc34898",
      "metadata": {
        "id": "a057289b-0645-4142-8db2-cc823cc34898"
      },
      "outputs": [],
      "source": [
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
      "metadata": {
        "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
        "outputId": "028ae5c4-3e65-4794-e4ca-89de969caafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "Get the count of competition hosts by theme.\n",
            "ASSISTANT: SELECT COUNT(Hosts), Theme FROM farm_competition GROUP BY Theme</s>\n"
          ]
        }
      ],
      "source": [
        "print(tokr.batch_decode(res)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991d4a93-dab8-4777-82d2-301c494deba0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "991d4a93-dab8-4777-82d2-301c494deba0"
      },
      "source": [
        "## [llama.cpp](https://github.com/abetlen/llama-cpp-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069",
      "metadata": {
        "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069"
      },
      "source": [
        "[TheBloke/Llama-2-7b-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f33831f-8c71-47dc-a131-9a6733d68198",
      "metadata": {
        "id": "6f33831f-8c71-47dc-a131-9a6733d68198"
      },
      "outputs": [],
      "source": [
        "!pip install llama_cpp_python -qq\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
        "outputId": "c6a951e8-70f6-4027-f580-0ca71b8f57a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d14fb2855a76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, seed, n_ctx, n_batch, n_gpu_layers, main_gpu, tensor_split, rope_freq_base, rope_freq_scale, low_vram, mul_mat_q, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, last_n_tokens_size, lora_base, lora_path, numa, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model path does not exist: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model path does not exist: /home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=\"content/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
      "metadata": {
        "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
        "outputId": "3fe3618d-1edd-4ae1-918d-c0842542addc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   192.25 ms\n",
            "llama_print_timings:      sample time =    14.98 ms /    32 runs   (    0.47 ms per token,  2135.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   192.16 ms /    15 tokens (   12.81 ms per token,    78.06 tokens per second)\n",
            "llama_print_timings:        eval time =   767.74 ms /    31 runs   (   24.77 ms per token,    40.38 tokens per second)\n",
            "llama_print_timings:       total time =  1032.79 ms\n"
          ]
        }
      ],
      "source": [
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
      "metadata": {
        "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
        "outputId": "a31baa72-3c5f-41e7-b2f5-541505766ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'text': 'Q: Name the planets in the solar system? A: 1. Pluto (no longer considered a planet) 2. Mercury 3. Venus 4. Earth 5. Mars 6.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
          ]
        }
      ],
      "source": [
        "print(output['choices'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}