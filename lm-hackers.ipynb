{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santiago-R/aupa.ai/blob/main/lm-hackers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [A hacker's guide to Language Models](https://colab.research.google.com/github/fastai/lm-hackers/blob/main/lm-hackers.ipynb#scrollTo=0b017bfc-5be0-4e41-9fa1-9f685c3b0de5)\n"
      ],
      "metadata": {
        "id": "8PCrJ_dZIOiQ"
      },
      "id": "8PCrJ_dZIOiQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "pbMRhlx6ffxA"
      },
      "id": "pbMRhlx6ffxA"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # , force_remount=True)"
      ],
      "metadata": {
        "id": "lKUyTtKkduDt",
        "outputId": "d60d010f-631b-45ed-e6c7-ae0007676b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lKUyTtKkduDt",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tokenize\n",
        "# from io import BytesIO"
      ],
      "metadata": {
        "id": "mnQzskuV91G4"
      },
      "id": "mnQzskuV91G4",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# path = Path(\"/content/drive/MyDrive/LLM\")"
      ],
      "metadata": {
        "id": "r6yIdsdmfkCS"
      },
      "id": "r6yIdsdmfkCS",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "101f8f63-6a03-49c0-81b8-9173563f7420",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "101f8f63-6a03-49c0-81b8-9173563f7420"
      },
      "source": [
        "## The OpenAI API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OpenAI api key as environvent variable (from Drive's api_keys.env)\n",
        "!pip install python-dotenv -qq\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path='/content/drive/MyDrive/LLM/api_keys.env')"
      ],
      "metadata": {
        "id": "pHmWz-Hv_bMg",
        "outputId": "73978288-5540-4a56-e70a-814f1d4c7882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pHmWz-Hv_bMg",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65",
      "metadata": {
        "id": "6564eb90-4a24-4a48-b898-5d4408ac2a65"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qq\n",
        "from openai import ChatCompletion, Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "ef432d50-54c5-46da-af51-b01488b7983f",
      "metadata": {
        "id": "ef432d50-54c5-46da-af51-b01488b7983f"
      },
      "outputs": [],
      "source": [
        "aussie_sys = \"You are an Aussie LLM that uses Aussie slang and analogies whenever possible.\"\n",
        "question = \"What is money?\"\n",
        "\n",
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": question}])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db35a96f-20a1-4643-a622-91e35a03ab0b",
      "metadata": {
        "id": "db35a96f-20a1-4643-a622-91e35a03ab0b"
      },
      "source": [
        "- [Model options](https://platform.openai.com/docs/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f",
      "metadata": {
        "id": "49ec8f04-b8f6-47e2-9b69-bdc186c3265f"
      },
      "outputs": [],
      "source": [
        "def response(c):\n",
        "    try:\n",
        "        return c['choices'][0]['message']['content']\n",
        "    except KeyError:\n",
        "        return c['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
      "metadata": {
        "id": "ef4af38f-d0d1-4982-95ed-567bf1c8ddf8",
        "outputId": "2e4a9519-7a26-4f7d-b672-e7d30d50df5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Righto mate, money is what makes the world go round, like a kangaroo hopping straight ahead. It's a medium of exchange, a tool used to buy and sell goods and services. Think of it as the fuel for your barbie, the cash you need to sling for snaggers and bevvies. Money can come in different forms, be it coins or notes, and nowadays, it can even be digital, like a dingo on a computer screen. Basically, money is the currency that keeps the economy dinkum, helping us to trade and get the things we need to survive and enjoy life.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
      "metadata": {
        "id": "50c8027b-937b-4076-a483-164d9f8e7c8b",
        "outputId": "603566a6-317e-4ef5-8ce3-959fa5c0f84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"prompt_tokens\": 31,\n",
            "  \"completion_tokens\": 125,\n",
            "  \"total_tokens\": 156\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(c.usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
      "metadata": {
        "id": "3a86ac95-1a95-436d-867a-e3b7c0e7b066",
        "outputId": "85586d27-85e7-446c-d78a-6358cb811865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "0.002 / 1000 * 150  # GPT 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
      "metadata": {
        "id": "c48fda82-ec55-4038-b083-383cead8b2c0",
        "outputId": "0cc4ec47-90a4-4f18-8350-1969769a1e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0045"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "0.03 / 1000 * 150  # GPT 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4",
      "metadata": {
        "id": "7750c720-4d77-464e-a0ec-fbb94f5889a4"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"system\", \"content\": aussie_sys},\n",
        "              {\"role\": \"user\", \"content\": \"What is money?\"},\n",
        "              {\"role\": \"assistant\", \"content\": \"Well, mate, money is like kangaroos actually.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Really? In what way?\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
      "metadata": {
        "id": "9d96eb95-2305-4e18-920f-4a3398a19b2c",
        "outputId": "edc45978-6766-4113-b837-79a2cfae2393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ah, I'm glad you asked, cobber! Money is like kangaroos because they both have value and jump around from one hand to another. Just as kangaroos can hop from place to place, money can be used for various things and can change hands pretty darn quick. And just like how people chase after kangaroos to get a glimpse, folks often chase after money to meet their needs and wants. So you see, money and kangaroos may be different critters, but they sure have a lot in common!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "91029d40-8360-4d0f-9f42-9ac7498cf417",
      "metadata": {
        "id": "91029d40-8360-4d0f-9f42-9ac7498cf417"
      },
      "outputs": [],
      "source": [
        "def askgpt(user, system=None, model=\"gpt-3.5-turbo\", **kwargs):\n",
        "    msgs = []\n",
        "    if system: msgs.append({\"role\": \"system\", \"content\": system})\n",
        "    msgs.append({\"role\": \"user\", \"content\": user})\n",
        "    return ChatCompletion.create(model=model, messages=msgs, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
      "metadata": {
        "id": "71de3f5c-8f1f-4e26-8260-493574a9f0e9",
        "outputId": "06229908-0251-4fcb-b704-19bd89dc0027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Well, mate, that's a real ripper of a question you've got there! The meaning of life is like chasing the perfect waveâ€”it's different for everyone. Some reckon it's all about finding true love and building a happy family, while others believe it's about seeking knowledge and making a difference in the world. The key is to find what gives your earthly existence some fair dinkum purpose and chase it like a kangaroo on steroids! Just remember, it's all about living the life you love, and loving the life you live. Cheers, mate!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "response(askgpt('What is the meaning of life?', system=aussie_sys))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99",
      "metadata": {
        "id": "07e79a25-571e-4096-8ad5-78d43fcb5b99"
      },
      "source": [
        "- [Limits](https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api)\n",
        "\n",
        "Created by Bing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6",
      "metadata": {
        "id": "ceeb0e7a-1d9d-48a5-8f51-e1a704115fb6"
      },
      "outputs": [],
      "source": [
        "def call_api(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    try: return ChatCompletion.create(model=model, messages=msgs)\n",
        "    except openai.error.RateLimitError as e:\n",
        "        retry_after = int(e.headers.get(\"retry-after\", 60))\n",
        "        print(f\"Rate limit exceeded, waiting for {retry_after} seconds...\")\n",
        "        time.sleep(retry_after)\n",
        "        return call_api(params, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
      "metadata": {
        "id": "447702a6-ec5a-4890-bbc0-61c6c37a425c",
        "outputId": "5c81362b-b6f6-4a8c-a784-97b05a125406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The world\\'s funniest joke is subjective and can vary from person to person, as humor is influenced by personal preferences and cultural differences. However, there have been scientific studies and attempts to find jokes that appeal to a wide range of people. \\n\\nOne notable study conducted in 2002 by psychologist Richard Wiseman involved over 40,000 participants from 70 different countries. He created an online experiment called \"LaughLab\" where people could rate and submit jokes. After analyzing the data, one of the jokes that ranked highly was:\\n\\n\"Why don\\'t some couples go to the gym? Because some relationships don\\'t work out!\"\\n\\nWhile this joke was popular in the study, it might not be universally considered the funniest to everyone. Humor is subjective and can vary greatly, so what may be hilarious to some can be less amusing to others. Ultimately, the perception of what is funny depends on individual tastes and preferences.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "response(call_api(\"What's the world's funniest joke? Has there ever been any scientific analysis?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "739e1549-687b-4216-bdf0-175e60c57401",
      "metadata": {
        "id": "739e1549-687b-4216-bdf0-175e60c57401"
      },
      "outputs": [],
      "source": [
        "c = Completion.create(prompt=\"Australian Jeremy Howard is \",\n",
        "                      model=\"gpt-3.5-turbo-instruct\", echo=True, logprobs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response(c)"
      ],
      "metadata": {
        "id": "HvAnyswSduzm",
        "outputId": "4d64e8be-fdd7-4986-a54e-6f4ff4ff5834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "id": "HvAnyswSduzm",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Australian Jeremy Howard is the former head of Kaggle, and the youngest ever data scientist as a university'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e86e7d-64f0-411d-bfce-289b06174dd4",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "98e86e7d-64f0-411d-bfce-289b06174dd4"
      },
      "source": [
        "## Create our own code interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785",
      "metadata": {
        "id": "1bb4fd89-8f24-41c0-a8e0-79633dcd1785"
      },
      "outputs": [],
      "source": [
        "from pydantic import create_model\n",
        "import inspect, json\n",
        "from inspect import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68",
      "metadata": {
        "id": "15a7f5a5-8dc0-4c0e-b141-9a0e35068c68"
      },
      "outputs": [],
      "source": [
        "def sums(a:int, b:int=1):\n",
        "    \"Adds a + b\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5afad9cb-1330-478b-8701-8d275bc1f985",
      "metadata": {
        "id": "5afad9cb-1330-478b-8701-8d275bc1f985"
      },
      "outputs": [],
      "source": [
        "def schema(f):\n",
        "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
        "          for n,o in inspect.signature(f).parameters.items()}\n",
        "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
        "    return dict(name=f.__name__, description=f.__doc__, parameters=s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
      "metadata": {
        "id": "d9af1e46-c560-4bc9-a6a7-78fafb2dfc78",
        "outputId": "7aad01df-7671-4291-83bc-829cb03c77fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'sums',\n",
              " 'description': 'Adds a + b',\n",
              " 'parameters': {'title': 'Input for `sums`',\n",
              "  'type': 'object',\n",
              "  'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
              "   'b': {'title': 'B', 'default': 1, 'type': 'integer'}},\n",
              "  'required': ['a']}}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "schema(sums)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453",
      "metadata": {
        "id": "1d60dcd4-514f-4efd-8b4a-fafac92c7453"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"Use the `sum` function to solve this: What is 6+3?\",\n",
        "           system = \"You must use the `sum` function instead of adding yourself.\",\n",
        "           functions=[schema(sums)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "id": "Z5mfzxAM5UUb",
        "outputId": "f43b5ad4-38d3-4a7d-f627-3e174458f60a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z5mfzxAM5UUb",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-84Sg5UyGVQchIOmn3fjpOuKTWxhW8 at 0x7f4702aebf60> JSON: {\n",
              "  \"id\": \"chatcmpl-84Sg5UyGVQchIOmn3fjpOuKTWxhW8\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1696074241,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": null,\n",
              "        \"function_call\": {\n",
              "          \"name\": \"sums\",\n",
              "          \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "        }\n",
              "      },\n",
              "      \"finish_reason\": \"function_call\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 83,\n",
              "    \"completion_tokens\": 22,\n",
              "    \"total_tokens\": 105\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
      "metadata": {
        "id": "5cc8a9eb-64fc-4ce3-9f93-aee334fc8c65",
        "outputId": "cdc63d72-ecb5-4dca-d3ed-cfb056cfe5bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject at 0x7f47029b8270> JSON: {\n",
              "  \"role\": \"assistant\",\n",
              "  \"content\": null,\n",
              "  \"function_call\": {\n",
              "    \"name\": \"sums\",\n",
              "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "m = c.choices[0].message\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
      "metadata": {
        "id": "5a881835-83a2-4fbc-9796-409831dcdb36",
        "outputId": "d1a62f9b-2b81-49ac-de3c-444ecd8b3d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"a\": 6,\n",
            "  \"b\": 3\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "k = m.function_call.arguments\n",
        "print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4",
      "metadata": {
        "id": "3f32b172-7730-4c58-a999-5d3800e8c2a4"
      },
      "outputs": [],
      "source": [
        "funcs_ok = {'sums', 'python'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "9853a78e-8643-4b5b-9b4a-a360c8443344",
      "metadata": {
        "id": "9853a78e-8643-4b5b-9b4a-a360c8443344"
      },
      "outputs": [],
      "source": [
        "def call_func(c):\n",
        "    fc = c.choices[0].message.function_call\n",
        "    if fc.name not in funcs_ok: return print(f'Not allowed: {fc.name}')\n",
        "    f = globals()[fc.name]\n",
        "    return f(**json.loads(fc.arguments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
      "metadata": {
        "id": "ce6be825-44fa-4bc9-a83a-7ecbcb6c2729",
        "outputId": "c29c9e9d-b0ec-4eef-97a9-bd66430e409c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2",
      "metadata": {
        "id": "a5ebf08c-2c3f-4c73-bd41-2f0d0a6ca3c2"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def run(code):\n",
        "    tree = ast.parse(code)\n",
        "    last_node = tree.body[-1] if tree.body else None\n",
        "\n",
        "    # If the last node is an expression, modify the AST to capture the result\n",
        "    if isinstance(last_node, ast.Expr):\n",
        "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
        "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
        "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
        "\n",
        "    ns = {}\n",
        "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
        "    return ns.get('_result', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "37aa6983-d9af-40e8-b438-ed8573399020",
      "metadata": {
        "id": "37aa6983-d9af-40e8-b438-ed8573399020",
        "outputId": "b28b4138-485a-47e8-a8d4-05e3e559d188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "run(\"\"\"\n",
        "a=1\n",
        "b=2\n",
        "a+b\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d",
      "metadata": {
        "id": "d2074acd-8a49-443b-89c6-cfc689ecad6d"
      },
      "outputs": [],
      "source": [
        "def python(code:str):\n",
        "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
        "    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
        "    if go.lower()!='y': return '#FAIL#'\n",
        "    return run(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b",
      "metadata": {
        "id": "02760caf-6d0d-4f5d-a7c6-cd8df5ee1a4b"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
      "metadata": {
        "id": "5fe5d796-c602-471f-bb58-bb876039bc39",
        "outputId": "0a779824-9af9-4a56-b56f-4389964bc49c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proceed with execution?\n",
            "```\n",
            "import math\n",
            "math.factorial(12)\n",
            "```\n",
            "y\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479001600"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "call_func(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176",
      "metadata": {
        "id": "c4e6bcd6-c23e-4ae8-8d38-78dae20ca176"
      },
      "outputs": [],
      "source": [
        "c = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    functions=[schema(python)],\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is 12 factorial?\"},\n",
        "              {\"role\": \"function\", \"name\": \"python\", \"content\": \"479001600\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "9e666c90-698d-492b-b21d-f6210ae77767",
      "metadata": {
        "id": "9e666c90-698d-492b-b21d-f6210ae77767",
        "outputId": "6e9a41d0-736c-4911-937b-72d4c52e7bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'12 factorial, denoted as 12!, is equal to 479,001,600.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exec_schema = {'name': 'exec',\n",
        " 'description': 'Execute the given source Python code',\n",
        " 'parameters': {'title': 'Input for `exec`',\n",
        "  'type': 'object',\n",
        "  'properties': {'source': {'title': 'S', 'type': 'string'}},\n",
        "  'required': ['source']}}"
      ],
      "metadata": {
        "id": "pVDtEgUaDLGy"
      },
      "id": "pVDtEgUaDLGy",
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = askgpt(\"What is 12 factorial?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[exec_schema])"
      ],
      "metadata": {
        "id": "Wb5WKbbwEXNM"
      },
      "id": "Wb5WKbbwEXNM",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def code_response(c, repl=True):\n",
        "    code = json.loads(c['choices'][0]['message']['function_call']['arguments'])['source']\n",
        "    if repl:\n",
        "        code_body = '\\n'.join(code.split('\\n')[:-1])\n",
        "        code_footer = code.split('\\n')[-1]\n",
        "        exec(code_body)\n",
        "        print(eval(code_footer))\n",
        "    else:\n",
        "        exec(code)\n",
        "        return"
      ],
      "metadata": {
        "id": "CZIE3OaTEXKP"
      },
      "id": "CZIE3OaTEXKP",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_response(c)"
      ],
      "metadata": {
        "id": "KRn2P4KoWNUh",
        "outputId": "761c751a-18bd-4cf4-aec8-1ff0eecd5149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KRn2P4KoWNUh",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "479001600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = c.choices[0].message.function_call\n",
        "f = globals()[fc.name]\n",
        "return f(**json.loads(fc.arguments))"
      ],
      "metadata": {
        "id": "8jx4ahsLSWM3"
      },
      "id": "8jx4ahsLSWM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "bef5cc05-0531-422b-9367-e7b90d784a54",
      "metadata": {
        "id": "bef5cc05-0531-422b-9367-e7b90d784a54"
      },
      "outputs": [],
      "source": [
        "c = askgpt(\"What is the capital of France?\",\n",
        "           system = \"Use python for any required computations.\",\n",
        "           functions=[schema(python)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
      "metadata": {
        "id": "112bc3db-5aa4-41ba-95df-1651916d7f0b",
        "outputId": "650f73ba-6dbf-4e86-fc75-50104e3adfdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is Paris.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "response(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1b7c7e1d-ad0c-4668-a10d-569ba6b7aa04"
      },
      "source": [
        "## PyTorch and Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f",
      "metadata": {
        "id": "1540a0c4-9266-4d7d-96a1-8d47d2fb737f"
      },
      "outputs": [],
      "source": [
        "from wikipediaapi import Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad",
      "metadata": {
        "id": "618d9520-54e7-4f31-8363-2cdd3c3668ad"
      },
      "outputs": [],
      "source": [
        "wiki = Wikipedia('JeremyHowardBot/0.0', 'en')\n",
        "jh_page = wiki.page('Jeremy_Howard_(entrepreneur)').text\n",
        "jh_page = jh_page.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
      "metadata": {
        "id": "53a0689a-ab59-431b-838d-33cae6f6500a",
        "outputId": "af99d29a-51e5-4e21-b5b7-ac126865c4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
            "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
            "Early in the COVID-19 epidemic he was a leading advocate for masking.\n",
            "\n",
            "Early life\n",
            "Howard was born in London, United Kingdom, and move\n"
          ]
        }
      ],
      "source": [
        "print(jh_page[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
      "metadata": {
        "id": "d3ded6a2-3665-416c-8d98-063f04aae0a0",
        "outputId": "009fa097-2449-4d64-fe94-fcdc50caceb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "613"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(jh_page.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b",
      "metadata": {
        "id": "f37d5fb1-7182-45a0-bd4e-6a39837ce20b"
      },
      "outputs": [],
      "source": [
        "ques_ctx = f\"\"\"Answer the question with the help of the provided context.\n",
        "\n",
        "## Context\n",
        "\n",
        "{jh_page}\n",
        "\n",
        "## Question\n",
        "\n",
        "{ques}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a",
      "metadata": {
        "id": "d6a7fb6d-d771-4eca-8e6f-29d90ade348a"
      },
      "outputs": [],
      "source": [
        "res = gen(mk_prompt(ques_ctx), 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
      "metadata": {
        "id": "85b10234-e1e6-4a1e-86a7-206e46e916f2",
        "outputId": "0ffda081-7c6c-41b3-a754-47d9bc5a522c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Jeremy Howard is an Australian data scientist, entrepreneur, and educator known for his work in deep learning. He is the co-founder of fast.ai, where he teaches courses, develops software, and conducts research in the field. Before co-founding fast.ai, he was the President and Chief Scientist of Kaggle, the CEO of Fastmail and Optimal Decisions Group, and has a background in management consulting.</s>\n"
          ]
        }
      ],
      "source": [
        "print(res[0].split('### Assistant:\\n')[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28",
      "metadata": {
        "id": "a0cdc342-a89c-4149-b3be-d4b5bf0f6f28"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e225ca13-1735-42e9-b162-66268fce5218",
      "metadata": {
        "id": "e225ca13-1735-42e9-b162-66268fce5218"
      },
      "outputs": [],
      "source": [
        "emb_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
      "metadata": {
        "id": "c636ed79-45ed-4728-9dca-1c86d282a1c4",
        "outputId": "60101e5b-882a-46a7-f6bd-16d9f87bf8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jeremy Howard (born 13 November 1973) is an Australian data scientist, entrepreneur, and educator.He is the co-founder of fast.ai, where he teaches introductory courses, develops software, and conducts research in the area of deep learning.\n",
            "Previously he founded and led Fastmail, Optimal Decisions Group, and Enlitic. He was President and Chief Scientist of Kaggle.\n",
            "Early in the COVID-19 epidemic he was a leading advocate for masking.\n"
          ]
        }
      ],
      "source": [
        "jh = jh_page.split('\\n\\n')[0]\n",
        "print(jh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed",
      "metadata": {
        "id": "1b0ad4ca-499a-4454-a215-4f5a676d68ed"
      },
      "outputs": [],
      "source": [
        "tb_page = wiki.page('Tony_Blair').text.split('\\nReferences\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
      "metadata": {
        "id": "26b28c6d-13ef-421a-865d-c311fb9fb2ae",
        "outputId": "d93a72e7-d0d9-41cd-95f5-54568c3cca70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sir Anthony Charles Lynton Blair  (born 6 May 1953) is a British politician who served as Prime Minister of the United Kingdom from 1997 to 2007 and Leader of the Labour Party from 1994 to 2007. He served as Leader of the Opposition from 1994 to 1997 and had various shadow cabinet posts from 1987 to 1994. Blair was Member of Parliament (MP) for Sedgefield from 1983 to 2007. He \n"
          ]
        }
      ],
      "source": [
        "tb = tb_page.split('\\n\\n')[0]\n",
        "print(tb[:380])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3173701f-1533-4eef-b080-3b771f2ba031",
      "metadata": {
        "id": "3173701f-1533-4eef-b080-3b771f2ba031"
      },
      "outputs": [],
      "source": [
        "q_emb,jh_emb,tb_emb = emb_model.encode([ques,jh,tb], convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
      "metadata": {
        "id": "6237634d-3708-477d-97e3-34dd7900f4eb",
        "outputId": "3c7a1380-dd27-4ead-bcda-ffdee93974f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([384])"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tb_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0",
      "metadata": {
        "id": "19638b95-f0dd-4605-8b04-3a0f9f9944b0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
      "metadata": {
        "id": "8a3937ac-45b7-4635-9f93-d1bcd1d6a016",
        "outputId": "4a805aac-a8ca-473b-d3c7-bec7b540ab96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7991, device='cuda:0')"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cosine_similarity(q_emb, jh_emb, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
      "metadata": {
        "id": "6c8beb5e-4a8c-4049-b42b-3acce573dd59",
        "outputId": "55679e83-03fd-4a09-919d-fd8aaf88f1f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5315, device='cuda:0')"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cosine_similarity(q_emb, tb_emb, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8473a6bc-fd25-4698-99d5-864ed1d5b40b"
      },
      "source": [
        "### Private GPTs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037bc313-052d-46b3-8938-fef988126dcb",
      "metadata": {
        "id": "037bc313-052d-46b3-8938-fef988126dcb"
      },
      "source": [
        "- [Sooo many](https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1948c419-4f04-4832-848c-a0c52dcc6a90",
      "metadata": {
        "id": "1948c419-4f04-4832-848c-a0c52dcc6a90"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8",
      "metadata": {
        "id": "03c2871f-e0fe-4891-b68e-05b6ca7373d8"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f",
      "metadata": {
        "id": "54366155-5ac3-4e75-8638-9e2ecf263c2f"
      },
      "source": [
        "[knowrohit07/know_sql](https://huggingface.co/datasets/knowrohit07/know_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed118caf-3e94-4773-a182-f2d346ced836",
      "metadata": {
        "id": "ed118caf-3e94-4773-a182-f2d346ced836"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset('knowrohit07/know_sql', revision='f33425d13f9e8aab1b46fa945326e9356d6d5726')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
      "metadata": {
        "id": "6d444ac3-a49f-4fcf-bcf4-c12c410d7aba",
        "outputId": "8e283317-2ed8-480c-930a-69274f972721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['context', 'answer', 'question'],\n",
              "        num_rows: 78562\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
      "metadata": {
        "id": "8ec155bd-5eb8-4eef-bdbc-b430a26fd11e",
        "outputId": "dd29624f-ea3d-485a-97e3-e22033731d70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'What are the hosts of competitions whose theme is not \"Aliens\"?'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trn = ds['train']\n",
        "trn[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e",
      "metadata": {
        "id": "e23cb868-10e4-4fcc-b8ba-9178ef3dac7e"
      },
      "source": [
        "`accelerate launch -m axolotl.cli.train sql.yml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
      "metadata": {
        "id": "ae6a579c-086e-46e4-a184-de5b71f4d80b",
        "outputId": "df3e65ba-e0b9-442e-b219-d38786e901f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)',\n",
              " 'answer': \"SELECT Hosts FROM farm_competition WHERE Theme <> 'Aliens'\",\n",
              " 'question': 'Get the count of competition hosts by theme.'}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tst = dict(**trn[3])\n",
        "tst['question'] = 'Get the count of competition hosts by theme.'\n",
        "tst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd",
      "metadata": {
        "id": "229c5fb9-0dff-460c-af1a-192fdec26ecd"
      },
      "outputs": [],
      "source": [
        "fmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n",
        "\n",
        "USER: {}\n",
        "===\n",
        "{}\n",
        "ASSISTANT:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2",
      "metadata": {
        "id": "feca892e-64f0-48d2-832e-0d8ceb9855d2"
      },
      "outputs": [],
      "source": [
        "def sql_prompt(d): return fmt.format(d[\"context\"], d[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
      "metadata": {
        "id": "058b0f4a-8fe0-4612-8584-451e8d04fe2d",
        "outputId": "1b575504-6d8d-4f02-fa8d-d0415d8e4ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "List all competition hosts sorted in ascending order.\n",
            "ASSISTANT:\n"
          ]
        }
      ],
      "source": [
        "print(sql_prompt(tst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86827af0-6cf0-43d0-b50a-fe080de83f48",
      "metadata": {
        "id": "86827af0-6cf0-43d0-b50a-fe080de83f48"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01",
      "metadata": {
        "id": "e6d33b0c-4973-4bc8-beef-aae7915f4b01"
      },
      "outputs": [],
      "source": [
        "ax_model = '/home/jhoward/git/ext/axolotl/qlora-out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7082fe-b668-4f11-abf8-548996ea3004",
      "metadata": {
        "id": "7b7082fe-b668-4f11-abf8-548996ea3004"
      },
      "outputs": [],
      "source": [
        "tokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54",
      "metadata": {
        "id": "fec3b5ad-0577-4867-b2d8-9143800c9f54"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
        "                                             torch_dtype=torch.bfloat16, device_map=0)\n",
        "model = PeftModel.from_pretrained(model, ax_model)\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('sql-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58bc69-6ea6-416a-baa9-db3338451d60",
      "metadata": {
        "id": "8a58bc69-6ea6-416a-baa9-db3338451d60"
      },
      "outputs": [],
      "source": [
        "toks = tokr(sql_prompt(tst), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a057289b-0645-4142-8db2-cc823cc34898",
      "metadata": {
        "id": "a057289b-0645-4142-8db2-cc823cc34898"
      },
      "outputs": [],
      "source": [
        "res = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
      "metadata": {
        "id": "80db7d77-89e2-4006-a1b2-78b807de1a68",
        "outputId": "028ae5c4-3e65-4794-e4ca-89de969caafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> SYSTEM: Use the following contextual information to concisely answer the question.\n",
            "\n",
            "USER: CREATE TABLE farm_competition (Hosts VARCHAR, Theme VARCHAR)\n",
            "===\n",
            "Get the count of competition hosts by theme.\n",
            "ASSISTANT: SELECT COUNT(Hosts), Theme FROM farm_competition GROUP BY Theme</s>\n"
          ]
        }
      ],
      "source": [
        "print(tokr.batch_decode(res)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991d4a93-dab8-4777-82d2-301c494deba0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "991d4a93-dab8-4777-82d2-301c494deba0"
      },
      "source": [
        "## [llama.cpp](https://github.com/abetlen/llama-cpp-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069",
      "metadata": {
        "id": "0dc9d6e4-5b25-4d11-8748-dc4f0bc98069"
      },
      "source": [
        "[TheBloke/Llama-2-7b-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f33831f-8c71-47dc-a131-9a6733d68198",
      "metadata": {
        "id": "6f33831f-8c71-47dc-a131-9a6733d68198"
      },
      "outputs": [],
      "source": [
        "!pip install llama_cpp_python -qq\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "3b824b6c-96cd-4bfe-a615-b3ba4543a92d",
        "outputId": "c6a951e8-70f6-4027-f580-0ca71b8f57a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d14fb2855a76>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, seed, n_ctx, n_batch, n_gpu_layers, main_gpu, tensor_split, rope_freq_base, rope_freq_scale, low_vram, mul_mat_q, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, last_n_tokens_size, lora_base, lora_path, numa, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model path does not exist: {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model path does not exist: /home/jhoward/git/llamacpp/llama-2-7b-chat.Q4_K_M.gguf"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=\"content/llamacpp/llama-2-7b-chat.Q4_K_M.gguf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
      "metadata": {
        "id": "c95a95b9-7c9f-499a-a72a-ad626da8c3f5",
        "outputId": "3fe3618d-1edd-4ae1-918d-c0842542addc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   192.25 ms\n",
            "llama_print_timings:      sample time =    14.98 ms /    32 runs   (    0.47 ms per token,  2135.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   192.16 ms /    15 tokens (   12.81 ms per token,    78.06 tokens per second)\n",
            "llama_print_timings:        eval time =   767.74 ms /    31 runs   (   24.77 ms per token,    40.38 tokens per second)\n",
            "llama_print_timings:       total time =  1032.79 ms\n"
          ]
        }
      ],
      "source": [
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
      "metadata": {
        "id": "f9f89cec-5453-4d67-85b2-c937ceb03a54",
        "outputId": "a31baa72-3c5f-41e7-b2f5-541505766ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'text': 'Q: Name the planets in the solar system? A: 1. Pluto (no longer considered a planet) 2. Mercury 3. Venus 4. Earth 5. Mars 6.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
          ]
        }
      ],
      "source": [
        "print(output['choices'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}